{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f4c81-0587-4b65-9548-cd004826e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import docx\n",
    "from docx import Document\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "WORD_DOC_PATH = \n",
    "OUTPUT_JSON_PATH =\n",
    "OUTPUT_DOCX_PATH =\n",
    "CHUNKS_COUNT = 8  # Split the document into 8 chunks for processing\n",
    "\n",
    "# Set up API key\n",
    "if \"DEEPSEEK_API_KEY\" not in os.environ:\n",
    "    os.environ[\"DEEPSEEK_API_KEY\"] = input(\"Enter your Deepseek API Key: \").strip()\n",
    "\n",
    "# Create Deepseek client\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "    base_url=\"https://api.deepseek.com/v1\"\n",
    ")\n",
    "\n",
    "def load_entities_from_docx(doc_path):\n",
    "    \"\"\"Load entity list from Word document\"\"\"\n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "        entities = []\n",
    "        \n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if text and not text.startswith(\"#\") and not text.startswith(\"Step\"):\n",
    "                # Remove numbering if present (e.g., \"1.Entity\" -> \"Entity\")\n",
    "                if re.match(r'^\\d+\\.', text):\n",
    "                    text = text.split('.', 1)[1].strip()\n",
    "                entities.append(text)\n",
    "                \n",
    "        print(f\"‚úÖ Successfully loaded {len(entities)} entities from document\")\n",
    "        return entities\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading document: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def split_into_chunks(entities, num_chunks):\n",
    "    \"\"\"Split the entity list into specified number of chunks\"\"\"\n",
    "    chunk_size = len(entities) // num_chunks\n",
    "    remainder = len(entities) % num_chunks\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Add one extra item to some chunks if there's a remainder\n",
    "        end = start + chunk_size + (1 if i < remainder else 0)\n",
    "        chunks.append(entities[start:end])\n",
    "        start = end\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "def process_entity_chunk(entities_chunk, chunk_index):\n",
    "    \"\"\"Process a chunk of entities using Deepseek API\"\"\"\n",
    "    # Format the entity list for the prompt\n",
    "    entities_text = \"\\n\".join([f\"{i+1}.{entity}\" for i, entity in enumerate(entities_chunk)])\n",
    "    \n",
    "    # Create the prompt based on the system role and requirements\n",
    "    prompt = f\"\"\"\n",
    "You are an Entity-Resolution Specialist in concrete material science. Your job is to take a raw list of terms and return‚Äîin strict JSON array form‚Äîone unique, standardized Canonical Name for every distinct concept.\n",
    "Workflow (in order)\n",
    "1.\tAnalyze each term\n",
    "Recognize domain items: cementitious materials, concrete types, properties, reactions, processes and failure modes.\n",
    "2.\tGroup equivalent terms\n",
    "Treat as identical when they differ only by\n",
    "a)\tsynonyms or jargon (‚ÄúMicro silica‚Äù = ‚ÄúSilica Fume‚Äù)\n",
    "b)\tabbreviations (‚ÄúSF‚Äù = ‚ÄúSilica Fume‚Äù)\n",
    "c)\tcase, spelling, singular/plural, minor wording.\n",
    "3.\tSelect one Canonical Name per group\n",
    "a)\tChoose the standard, unambiguous term used in current literature.\n",
    "b)\tPrefer full names over acronyms‚Äîexcept universally accepted ones (e.g., ‚ÄúUHPC‚Äù).\n",
    "c)\tUse the singular form.\n",
    "d)\tRender in Title Case (‚ÄúSilica Fume‚Äù, ‚ÄúCompressive Strength‚Äù).\n",
    "e)\tUngrouped terms: normalize with rules 3 & 4 and keep.\n",
    "4.\tReturn output\n",
    "\n",
    "Your output must be a valid JSON array of strings only, alphabetically sorted, with no additional explanation.\n",
    "\n",
    "Example:\n",
    "Input: \n",
    "1.Silica Fume  \n",
    "2.Microsilica  \n",
    "3.SF  \n",
    "4.Compressive strength  \n",
    "5.Compressive Strength  \n",
    "6.compression strength  \n",
    "7.UHPC  \n",
    "8.Fly Ash  \n",
    "9.fly ash  \n",
    "10.Curing Process  \n",
    "11.Aggregate  \n",
    "12.Aggregates  \n",
    "13.Silica Fume\n",
    "\n",
    "Expected Output:\n",
    "[\"Aggregate\", \"Compressive Strength\", \"Curing Process\", \"Fly Ash\", \"Silica Fume\", \"UHPC\"]\n",
    "\n",
    "IMPORTANT: Respond ONLY with the JSON array, beginning with [ and ending with ], nothing else.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(f\"üîÑ Processing chunk {chunk_index+1}/{CHUNKS_COUNT} ({len(entities_chunk)} entities)...\")\n",
    "        \n",
    "        # Call Deepseek API with retry logic\n",
    "        max_retries = 3\n",
    "        response = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"deepseek-chat\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.1,  # Lower temperature for more deterministic results\n",
    "                    max_tokens=4000,\n",
    "                    top_p=0.95,\n",
    "                    timeout=120  # Longer timeout for processing larger chunks\n",
    "                )\n",
    "                break  # Success, exit retry loop\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"‚ö†Ô∏è Attempt {attempt+1} failed: {str(e)}, retrying in 10 seconds...\")\n",
    "                    time.sleep(10)\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        if response:\n",
    "            response_content = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Extract JSON array from response\n",
    "            canonical_entities = extract_json_array(response_content)\n",
    "            \n",
    "            if canonical_entities:\n",
    "                print(f\"‚úÖ Successfully processed chunk {chunk_index+1}: Found {len(canonical_entities)} canonical entities\")\n",
    "                return canonical_entities\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Failed to extract valid JSON from chunk {chunk_index+1}\")\n",
    "                return []\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing chunk {chunk_index+1}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_json_array(text):\n",
    "    \"\"\"Extract JSON array from API response, with various fallback methods\"\"\"\n",
    "    # Try direct JSON parsing first\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove markdown code blocks if present\n",
    "    if \"```json\" in text:\n",
    "        text = re.sub(r'```json\\s*|\\s*```', '', text)\n",
    "    elif \"```\" in text:\n",
    "        text = re.sub(r'```\\s*|\\s*```', '', text)\n",
    "    \n",
    "    # Try direct parsing\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Try regex extraction\n",
    "    try:\n",
    "        array_pattern = r'\\[(.*?)\\]'\n",
    "        array_match = re.search(array_pattern, text, re.DOTALL)\n",
    "        \n",
    "        if array_match:\n",
    "            array_content = array_match.group(1)\n",
    "            # Extract string elements\n",
    "            strings_pattern = r'\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\"'\n",
    "            strings = re.findall(strings_pattern, array_content)\n",
    "            \n",
    "            if strings:\n",
    "                return strings\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Final attempt: manual extraction of quoted strings\n",
    "    try:\n",
    "        strings_pattern = r'\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\"'\n",
    "        strings = re.findall(strings_pattern, text)\n",
    "        if strings:\n",
    "            return strings\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return []\n",
    "\n",
    "def merge_canonical_entities(all_canonical_chunks):\n",
    "    \"\"\"Merge and deduplicate canonical entities from all chunks\"\"\"\n",
    "    # Flatten all chunks\n",
    "    all_entities = []\n",
    "    for chunk in all_canonical_chunks:\n",
    "        all_entities.extend(chunk)\n",
    "    \n",
    "    # Normalize and deduplicate\n",
    "    normalized_entities = {}\n",
    "    \n",
    "    for entity in all_entities:\n",
    "        # Normalize: convert to lowercase for comparison, but keep original case\n",
    "        normalized_key = entity.lower()\n",
    "        \n",
    "        # Keep the best version (prefer longer version if available)\n",
    "        if normalized_key not in normalized_entities or len(entity) > len(normalized_entities[normalized_key]):\n",
    "            normalized_entities[normalized_key] = entity\n",
    "    \n",
    "    # Get final deduplicated list and sort alphabetically\n",
    "    final_entities = sorted(normalized_entities.values())\n",
    "    \n",
    "    return final_entities\n",
    "\n",
    "def save_results(canonical_entities, json_path, docx_path):\n",
    "    \"\"\"Save results in both JSON and DOCX formats\"\"\"\n",
    "    # Save as JSON\n",
    "    try:\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(canonical_entities, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Saved {len(canonical_entities)} canonical entities to {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving JSON: {str(e)}\")\n",
    "    \n",
    "    # Save as DOCX\n",
    "    try:\n",
    "        doc = Document()\n",
    "        doc.add_heading(\"Canonical Entity Names for Concrete Materials Science\", level=1)\n",
    "        doc.add_paragraph(f\"Total unique entities: {len(canonical_entities)}\")\n",
    "        doc.add_paragraph(f\"Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Add a divider\n",
    "        doc.add_paragraph(\"=\" * 50)\n",
    "        \n",
    "        # Add the entities with numbers\n",
    "        for i, entity in enumerate(canonical_entities, 1):\n",
    "            doc.add_paragraph(f\"{i}. {entity}\")\n",
    "        \n",
    "        doc.save(docx_path)\n",
    "        print(f\"‚úÖ Saved canonical entities to Word document: {docx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving DOCX: {str(e)}\")\n",
    "\n",
    "def run_entity_resolution():\n",
    "    \"\"\"Main function to run the entity resolution process\"\"\"\n",
    "    print(\"üöÄ Starting Entity Resolution for Concrete Materials Science\")\n",
    "    \n",
    "    # Step 1: Load entities from Word document\n",
    "    entities = load_entities_from_docx(WORD_DOC_PATH)\n",
    "    if not entities:\n",
    "        print(\"‚ùå No entities found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Split entities into chunks\n",
    "    entity_chunks = split_into_chunks(entities, CHUNKS_COUNT)\n",
    "    print(f\"üìä Split {len(entities)} entities into {len(entity_chunks)} chunks\")\n",
    "    \n",
    "    # Step 3: Process each chunk\n",
    "    canonical_chunks = []\n",
    "    \n",
    "    for i, chunk in enumerate(entity_chunks):\n",
    "        canonical_entities = process_entity_chunk(chunk, i)\n",
    "        canonical_chunks.append(canonical_entities)\n",
    "        \n",
    "        # Add delay between chunks to avoid API rate limits\n",
    "        if i < len(entity_chunks) - 1:\n",
    "            print(\"‚è±Ô∏è Waiting 5 seconds before processing next chunk...\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    # Step 4: Merge results from all chunks\n",
    "    final_canonical_entities = merge_canonical_entities(canonical_chunks)\n",
    "    print(f\"üéØ Final result: {len(final_canonical_entities)} unique canonical entities\")\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    save_results(final_canonical_entities, OUTPUT_JSON_PATH, OUTPUT_DOCX_PATH)\n",
    "    \n",
    "    print(\"‚úÖ Entity Resolution process completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_entity_resolution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
