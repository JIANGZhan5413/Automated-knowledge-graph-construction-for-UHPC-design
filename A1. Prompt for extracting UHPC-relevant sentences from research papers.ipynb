{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48d3d6-5016-437d-b9fa-d26589cd4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pdfplumber\n",
    "import openai\n",
    "import getpass\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docx import Document\n",
    "import time\n",
    "\n",
    "# --- API Key Setup (unchanged) ---\n",
    "if \"DEEPSEEK_API_KEY\" not in os.environ:\n",
    "    os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Enter your Deepseek API Key: \")\n",
    "\n",
    "# --- PDF Extraction (unchanged) ---\n",
    "def extract_pdf_text(pdf_path):\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                full_text += text + \"\\n\"\n",
    "        return full_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æ— æ³•æ‰“å¼€æˆ–å¤„ç† PDF æ–‡ä»¶ {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Text Splitting (Updated Defaults) ---\n",
    "def split_text(text, chunk_size=25000, overlap=500): # Updated defaults\n",
    "    if not text:\n",
    "        return []\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"], # Added spaces after punctuation\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# --- Deepseek Processing (Updated max_tokens) ---\n",
    "def gpt_process_text(text_chunk):\n",
    "    # --- Prompt (unchanged) ---\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant responsible for extracting sentences relevant to Ultra-High Performance Concrete (UHPC) from research papers. Your task is to identify, clean, and extract relevant sentences while preserving their original structure.\n",
    "\n",
    "    **Sentence Selection and Cleaning Criteria:**\n",
    "    1ï¸âƒ£ Extract sentences that explicitly mention 'UHPC' or 'Ultra-High Performance Concrete' (case-insensitive).\n",
    "    2ï¸âƒ£ Extract sentences that discuss UHPC-related topics even if they do not contain 'UHPC'.\n",
    "    3ï¸âƒ£ Sentences should be relevant, not generic. Ignore vague statements like â€œConcrete has high strength.â€\n",
    "    4ï¸âƒ£ Sentences should be at least 8 words long to ensure meaningful content.\n",
    "    5ï¸âƒ£ Exclude references, captions, formulas, and table data to prevent noisy or incomplete extractions.\n",
    "    6ï¸âƒ£ Do not modify, rewrite, or summarize extracted sentences. Preserve their original wording, structure, and punctuation.\n",
    "    7ï¸âƒ£ If a sentence contains references like [1] or (2023), remove only the reference markers, keeping the rest of the sentence intact.\n",
    "    Return the cleaned and validated sentences as a JSON list. Ensure the JSON format is strictly adhered to.\n",
    "\n",
    "    **âœ… Output Format (Strict JSON Only):**\n",
    "    ```json\n",
    "    {{\n",
    "        \"valid_sentences\": [\"sentence1\", \"sentence2\", \"sentence3\"]\n",
    "    }}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": text_chunk}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=7800, # Updated max_tokens\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        gpt_output = response.choices[0].message.content.strip()\n",
    "\n",
    "        # --- JSON Parsing Logic (improved robustness slightly) ---\n",
    "        def parse_gpt_response(response_text):\n",
    "            try:\n",
    "                # Attempt direct JSON parsing first, as response_format should ensure it\n",
    "                return json.loads(response_text)\n",
    "            except json.JSONDecodeError as e:\n",
    "                 print(f\"âš ï¸ åˆæ­¥ JSON è§£æé”™è¯¯: {e}\")\n",
    "                 print(f\"   å°è¯•æ¸…ç†å’Œè§£æ...\")\n",
    "                 try:\n",
    "                    # Fallback: Clean potential markdown and leading/trailing garbage\n",
    "                    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        cleaned_text = json_match.group(0)\n",
    "                        result = json.loads(cleaned_text)\n",
    "                        print(\"   é€šè¿‡æ­£åˆ™æ¸…ç†åè§£ææˆåŠŸã€‚\")\n",
    "                        return result\n",
    "                    else:\n",
    "                        print(f\"   æ— æ³•åœ¨å“åº”ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„ JSON ç»“æ„: {response_text[:500]}...\")\n",
    "                        return None\n",
    "                 except json.JSONDecodeError as e2:\n",
    "                    print(f\"   æ¸…ç†å JSON è§£æä»ç„¶å¤±è´¥: {e2}\")\n",
    "                    print(f\"   æ”¶åˆ°å†…å®¹ (å‰500å­—ç¬¦): {response_text[:500]}...\")\n",
    "                    return None # Give up if parsing fails\n",
    "\n",
    "        result = parse_gpt_response(gpt_output)\n",
    "        return result.get(\"valid_sentences\", []) if result else []\n",
    "\n",
    "    # --- Error Handling (unchanged, but RateLimitError sleep is important) ---\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"âŒ Deepseek API è¿æ¥å¤±è´¥: {e}\")\n",
    "        return []\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"â³ Deepseek API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… 15 ç§’åé‡è¯•...\") # Increased wait time slightly\n",
    "        time.sleep(15)\n",
    "        return [] # Simplistic handling: skip chunk on rate limit for now\n",
    "    except openai.APIStatusError as e:\n",
    "        # Check for specific error codes, like context length exceeded (though unlikely with current chunk size)\n",
    "        # or output token limit exceeded\n",
    "        print(f\"âŒ Deepseek API çŠ¶æ€é”™è¯¯: {e.status_code}\")\n",
    "        print(f\"   å“åº”: {e.response}\")\n",
    "        # Potentially log e.response.text to see the error message from Deepseek\n",
    "        if \"maximum context length\" in str(e.response).lower():\n",
    "             print(\"   é”™è¯¯æç¤ºï¼šå¯èƒ½è¶…è¿‡äº†æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼\")\n",
    "        if \"maximum number of tokens\" in str(e.response).lower():\n",
    "             print(\"   é”™è¯¯æç¤ºï¼šå¯èƒ½è¶…è¿‡äº†æœ€å¤§è¾“å‡º token æ•°ï¼è¾“å‡ºå¯èƒ½è¢«æˆªæ–­ã€‚\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Deepseek å¤„ç†å¤±è´¥: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Saving to Doc (unchanged) ---\n",
    "def save_sentences_to_doc(sentences, doc_path):\n",
    "    try:\n",
    "        doc = Document()\n",
    "        doc.add_heading(\"Step1_Text Generation (DeepSeek - Optimized)\", level=1)\n",
    "        count = 0\n",
    "        if isinstance(sentences, list): # Ensure it's a list\n",
    "            for i, sentence in enumerate(sentences, 1):\n",
    "                if isinstance(sentence, str):\n",
    "                    doc.add_paragraph(f\"{i}. {sentence}\")\n",
    "                    count += 1\n",
    "                else:\n",
    "                    print(f\"âš ï¸ è·³è¿‡éå­—ç¬¦ä¸²é¡¹ç›®ç´¢å¼• {i}: {sentence}\")\n",
    "            doc.save(doc_path)\n",
    "            print(f\"âœ… å·²æˆåŠŸä¿å­˜ {count} ä¸ªå¥å­è‡³ {doc_path}\")\n",
    "        else:\n",
    "             print(f\"âŒ é¢„æœŸå¥å­åˆ—è¡¨ï¼Œä½†æ”¶åˆ°: {type(sentences)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜ Word æ–‡æ¡£å¤±è´¥: {e}\")\n",
    "\n",
    "# --- Main Program Logic (unchanged structure, uses updated defaults) ---\n",
    "def main(pdf_paths, output_doc_path):\n",
    "    all_relevant_sentences = []\n",
    "    total_files = len(pdf_paths)\n",
    "\n",
    "    for idx, pdf_path in enumerate(pdf_paths):\n",
    "        print(f\"\\nğŸš€ å¤„ç†æ–‡ä»¶ {idx+1}/{total_files}: {os.path.basename(pdf_path)}\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡: {pdf_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text = extract_pdf_text(pdf_path)\n",
    "            if not text:\n",
    "                print(f\"âš ï¸ æœªèƒ½ä» {os.path.basename(pdf_path)} æå–æ–‡æœ¬æˆ–æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡ã€‚\")\n",
    "                continue\n",
    "\n",
    "            print(\"ğŸ“– æ­£åœ¨æ‹†åˆ†æ–‡æœ¬ (chunk_size=25000, overlap=500)...\")\n",
    "            # Uses the new defaults from split_text definition\n",
    "            text_chunks = split_text(text)\n",
    "            print(f\"ğŸ”¹ æ–‡æœ¬åˆ†ä¸º {len(text_chunks)} ä¸ªéƒ¨åˆ†ã€‚\")\n",
    "\n",
    "            if not text_chunks:\n",
    "                 print(f\"âš ï¸ æ–‡æœ¬æ‹†åˆ†åä¸ºç©ºï¼Œè·³è¿‡æ–‡ä»¶ {os.path.basename(pdf_path)}ã€‚\")\n",
    "                 continue\n",
    "\n",
    "            chunk_sentences = []\n",
    "            for i, chunk in enumerate(text_chunks):\n",
    "                # Add check for chunk size just in case splitter produces large chunks\n",
    "                if len(chunk) > 30000: # Arbitrary safety check slightly above target\n",
    "                     print(f\"  âš ï¸ å— {i+1} å­—ç¬¦æ•°å¼‚å¸¸ ({len(chunk)})ï¼Œå¯èƒ½å¯¼è‡´é—®é¢˜ã€‚è·³è¿‡æ­¤å—ã€‚\")\n",
    "                     continue\n",
    "\n",
    "                print(f\"ğŸ¤– ä½¿ç”¨ Deepseek å¤„ç†ç¬¬ {i+1}/{len(text_chunks)} æ®µæ–‡æœ¬ (max_tokens=7800)...\")\n",
    "                sentences = gpt_process_text(chunk)\n",
    "                if sentences:\n",
    "                     chunk_sentences.extend(sentences)\n",
    "                time.sleep(1.5) # Slightly increase sleep time due to larger requests\n",
    "\n",
    "            print(f\"ğŸ“„ æ–‡ä»¶ {os.path.basename(pdf_path)} å¤„ç†å®Œæ¯•ï¼Œæ‰¾åˆ° {len(chunk_sentences)} ä¸ªç›¸å…³å¥å­ã€‚\")\n",
    "            all_relevant_sentences.extend(chunk_sentences)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å¤„ç†æ–‡ä»¶ {os.path.basename(pdf_path)} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nâœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼Œæ€»ç­›é€‰åçš„å¥å­æ•°é‡: {len(all_relevant_sentences)}\")\n",
    "\n",
    "    if all_relevant_sentences:\n",
    "        print(\"ğŸ“‘ ä¿å­˜ç»“æœåˆ°æ–‡æ¡£...\")\n",
    "        output_dir = os.path.dirname(output_doc_path)\n",
    "        try:\n",
    "            if output_dir and not os.path.exists(output_dir):\n",
    "                 os.makedirs(output_dir)\n",
    "                 print(f\"   åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}\")\n",
    "            save_sentences_to_doc(all_relevant_sentences, output_doc_path)\n",
    "        except Exception as e:\n",
    "             print(f\"âŒ åˆ›å»ºè¾“å‡ºç›®å½•æˆ–ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
    "    else:\n",
    "        print(\"â¹ï¸ æœªæå–åˆ°ç›¸å…³å¥å­ï¼Œä¸ç”Ÿæˆè¾“å‡ºæ–‡ä»¶ã€‚\")\n",
    "\n",
    "\n",
    "# --- Execution Block (Path generation unchanged) ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_base_dir = r\"D:\\SIT\\knowledge\\data\"\n",
    "    num_files = 50\n",
    "    if not os.path.isdir(pdf_base_dir):\n",
    "        print(f\"é”™è¯¯ï¼šæŒ‡å®šçš„ PDF åŸºç¡€ç›®å½•ä¸å­˜åœ¨: {pdf_base_dir}\")\n",
    "        exit()\n",
    "    pdf_files = [os.path.join(pdf_base_dir, f\"{i}.pdf\") for i in range(1, num_files + 1)]\n",
    "\n",
    "    output_doc = r\"D:\\SIT\\knowledge\\Result\\Step1_UHPC_Sentences_Extracted_DeepSeek_Optimized.docx\" # Updated output name\n",
    "\n",
    "    print(f\"å°†å¤„ç† {len(pdf_files)} ä¸ª PDF æ–‡ä»¶ä»ç›®å½•: {pdf_base_dir}\")\n",
    "    print(f\"ä½¿ç”¨è®¾ç½®: chunk_size=25000, overlap=500, max_tokens=7800\")\n",
    "    print(f\"ç»“æœå°†ä¿å­˜è‡³: {output_doc}\")\n",
    "\n",
    "    main(pdf_files, output_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
