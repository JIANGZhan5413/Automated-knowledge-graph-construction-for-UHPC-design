{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48d3d6-5016-437d-b9fa-d26589cd4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pdfplumber\n",
    "import openai\n",
    "import getpass\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docx import Document\n",
    "import time\n",
    "\n",
    "# --- API Key Setup (unchanged) ---\n",
    "if \"DEEPSEEK_API_KEY\" not in os.environ:\n",
    "    os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Enter your Deepseek API Key: \")\n",
    "\n",
    "# --- PDF Extraction (unchanged) ---\n",
    "def extract_pdf_text(pdf_path):\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                full_text += text + \"\\n\"\n",
    "        return full_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Unable to open or process PDF file {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Text Splitting (Updated Defaults) ---\n",
    "def split_text(text, chunk_size=25000, overlap=500): # Updated defaults\n",
    "    if not text:\n",
    "        return []\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"], # Added spaces after punctuation\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# --- Deepseek Processing (Updated max_tokens) ---\n",
    "def gpt_process_text(text_chunk):\n",
    "    # --- Prompt (unchanged) ---\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant responsible for extracting sentences relevant to Ultra-High Performance Concrete (UHPC) from research papers. Your task is to identify, clean, and extract relevant sentences while preserving their original structure.\n",
    "\n",
    "    **Sentence Selection and Cleaning Criteria:**\n",
    "    1Ô∏è‚É£ Extract sentences that explicitly mention 'UHPC' or 'Ultra-High Performance Concrete' (case-insensitive).\n",
    "    2Ô∏è‚É£ Extract sentences that discuss UHPC-related topics even if they do not contain 'UHPC'.\n",
    "    3Ô∏è‚É£ Sentences should be relevant, not generic. Ignore vague statements like ‚ÄúConcrete has high strength.‚Äù\n",
    "    4Ô∏è‚É£ Sentences should be at least 8 words long to ensure meaningful content.\n",
    "    5Ô∏è‚É£ Exclude references, captions, formulas, and table data to prevent noisy or incomplete extractions.\n",
    "    6Ô∏è‚É£ Do not modify, rewrite, or summarize extracted sentences. Preserve their original wording, structure, and punctuation.\n",
    "    7Ô∏è‚É£ If a sentence contains references like [1] or (2023), remove only the reference markers, keeping the rest of the sentence intact.\n",
    "    Return the cleaned and validated sentences as a JSON list. Ensure the JSON format is strictly adhered to.\n",
    "\n",
    "    **‚úÖ Output Format (Strict JSON Only):**\n",
    "    ```json\n",
    "    {{\n",
    "        \"valid_sentences\": [\"sentence1\", \"sentence2\", \"sentence3\"]\n",
    "    }}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": text_chunk}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=7800, # Updated max_tokens\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        gpt_output = response.choices[0].message.content.strip()\n",
    "\n",
    "        # --- JSON Parsing Logic (improved robustness slightly) ---\n",
    "        def parse_gpt_response(response_text):\n",
    "            try:\n",
    "                # Attempt direct JSON parsing first, as response_format should ensure it\n",
    "                return json.loads(response_text)\n",
    "            except json.JSONDecodeError as e:\n",
    "                 print(f\"‚ö†Ô∏è Initial JSON parsing error: {e}\")\n",
    "                 print(f\"   Attempting cleanup and re-parse...\")\n",
    "                 try:\n",
    "                    # Fallback: Clean potential markdown and leading/trailing garbage\n",
    "                    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        cleaned_text = json_match.group(0)\n",
    "                        result = json.loads(cleaned_text)\n",
    "                        print(\"   Successfully parsed after regex cleanup.\")\n",
    "                        return result\n",
    "                    else:\n",
    "                        print(f\"   Could not find valid JSON structure in response: {response_text[:500]}...\")\n",
    "                        return None\n",
    "                 except json.JSONDecodeError as e2:\n",
    "                    print(f\"   Still failed after cleanup: {e2}\")\n",
    "                    print(f\"   Received content (first 500 chars): {response_text[:500]}...\")\n",
    "                    return None # Give up if parsing fails\n",
    "\n",
    "        result = parse_gpt_response(gpt_output)\n",
    "        return result.get(\"valid_sentences\", []) if result else []\n",
    "\n",
    "    # --- Error Handling (unchanged, but RateLimitError sleep is important) ---\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"‚ùå Deepseek API connection failed: {e}\")\n",
    "        return []\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"‚è≥ Deepseek API rate limit, waiting 15 seconds before retry...\")\n",
    "        time.sleep(15)\n",
    "        return [] # Simplistic handling: skip chunk on rate limit for now\n",
    "    except openai.APIStatusError as e:\n",
    "        print(f\"‚ùå Deepseek API status error: {e.status_code}\")\n",
    "        print(f\"   Response: {e.response}\")\n",
    "        if \"maximum context length\" in str(e.response).lower():\n",
    "             print(\"   Hint: Possibly exceeded maximum context length!\")\n",
    "        if \"maximum number of tokens\" in str(e.response).lower():\n",
    "             print(\"   Hint: Possibly exceeded maximum output tokens! Output may be truncated.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Deepseek processing failed: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Saving to Doc (unchanged) ---\n",
    "def save_sentences_to_doc(sentences, doc_path):\n",
    "    try:\n",
    "        doc = Document()\n",
    "        doc.add_heading(\"Step1_Text Generation (DeepSeek - Optimized)\", level=1)\n",
    "        count = 0\n",
    "        if isinstance(sentences, list): # Ensure it's a list\n",
    "            for i, sentence in enumerate(sentences, 1):\n",
    "                if isinstance(sentence, str):\n",
    "                    doc.add_paragraph(f\"{i}. {sentence}\")\n",
    "                    count += 1\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipped non-string item at index {i}: {sentence}\")\n",
    "            doc.save(doc_path)\n",
    "            print(f\"‚úÖ Successfully saved {count} sentences to {doc_path}\")\n",
    "        else:\n",
    "             print(f\"‚ùå Expected a list of sentences, but got: {type(sentences)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save Word document: {e}\")\n",
    "\n",
    "# --- Main Program Logic (unchanged structure, uses updated defaults) ---\n",
    "def main(pdf_paths, output_doc_path):\n",
    "    all_relevant_sentences = []\n",
    "    total_files = len(pdf_paths)\n",
    "\n",
    "    for idx, pdf_path in enumerate(pdf_paths):\n",
    "        print(f\"\\nüöÄ Processing file {idx+1}/{total_files}: {os.path.basename(pdf_path)}\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"‚ö†Ô∏è File not found, skipping: {pdf_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text = extract_pdf_text(pdf_path)\n",
    "            if not text:\n",
    "                print(f\"‚ö†Ô∏è Failed to extract text from {os.path.basename(pdf_path)} or file is empty, skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(\"üìñ Splitting text (chunk_size=25000, overlap=500)...\")\n",
    "            text_chunks = split_text(text)\n",
    "            print(f\"üîπ Text split into {len(text_chunks)} chunks.\")\n",
    "\n",
    "            if not text_chunks:\n",
    "                 print(f\"‚ö†Ô∏è Text splitting produced empty result, skipping file {os.path.basename(pdf_path)}.\")\n",
    "                 continue\n",
    "\n",
    "            chunk_sentences = []\n",
    "            for i, chunk in enumerate(text_chunks):\n",
    "                if len(chunk) > 30000: # Arbitrary safety check slightly above target\n",
    "                     print(f\"  ‚ö†Ô∏è Chunk {i+1} has abnormal size ({len(chunk)} chars), may cause issues. Skipping.\")\n",
    "                     continue\n",
    "\n",
    "                print(f\"ü§ñ Using Deepseek to process chunk {i+1}/{len(text_chunks)} (max_tokens=7800)...\")\n",
    "                sentences = gpt_process_text(chunk)\n",
    "                if sentences:\n",
    "                     chunk_sentences.extend(sentences)\n",
    "                time.sleep(1.5) # Slightly increased sleep due to larger requests\n",
    "\n",
    "            print(f\"üìÑ Finished processing {os.path.basename(pdf_path)}, found {len(chunk_sentences)} relevant sentences.\")\n",
    "            all_relevant_sentences.extend(chunk_sentences)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error while processing {os.path.basename(pdf_path)}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n‚úÖ All files processed, total number of extracted sentences: {len(all_relevant_sentences)}\")\n",
    "\n",
    "    if all_relevant_sentences:\n",
    "        print(\"üìë Saving results to document...\")\n",
    "        output_dir = os.path.dirname(output_doc_path)\n",
    "        try:\n",
    "            if output_dir and not os.path.exists(output_dir):\n",
    "                 os.makedirs(output_dir)\n",
    "                 print(f\"   Created output directory: {output_dir}\")\n",
    "            save_sentences_to_doc(all_relevant_sentences, output_doc_path)\n",
    "        except Exception as e:\n",
    "             print(f\"‚ùå Error creating output directory or saving file: {e}\")\n",
    "    else:\n",
    "        print(\"‚èπÔ∏è No relevant sentences extracted, output file will not be generated.\")\n",
    "\n",
    "\n",
    "# --- Execution Block (Path generation unchanged) ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_base_dir = \n",
    "    num_files = 50\n",
    "    if not os.path.isdir(pdf_base_dir):\n",
    "        print(f\"Error: Specified PDF base directory does not exist: {pdf_base_dir}\")\n",
    "        exit()\n",
    "    pdf_files = [os.path.join(pdf_base_dir, f\"{i}.pdf\") for i in range(1, num_files + 1)]\n",
    "\n",
    "    output_doc =  # Updated output name\n",
    "\n",
    "    print(f\"Will process {len(pdf_files)} PDF files from directory: {pdf_base_dir}\")\n",
    "    print(f\"Using settings: chunk_size=25000, overlap=500, max_tokens=7800\")\n",
    "    print(f\"Results will be saved to: {output_doc}\")\n",
    "\n",
    "    main(pdf_files, output_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
