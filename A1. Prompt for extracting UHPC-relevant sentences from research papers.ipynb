{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48d3d6-5016-437d-b9fa-d26589cd4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pdfplumber\n",
    "import openai\n",
    "import getpass\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docx import Document\n",
    "import time\n",
    "\n",
    "# --- API Key Setup (unchanged) ---\n",
    "if \"DEEPSEEK_API_KEY\" not in os.environ:\n",
    "    os.environ[\"DEEPSEEK_API_KEY\"] = getpass.getpass(\"Enter your Deepseek API Key: \")\n",
    "\n",
    "# --- PDF Extraction (unchanged) ---\n",
    "def extract_pdf_text(pdf_path):\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                full_text += text + \"\\n\"\n",
    "        return full_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 无法打开或处理 PDF 文件 {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Text Splitting (Updated Defaults) ---\n",
    "def split_text(text, chunk_size=25000, overlap=500): # Updated defaults\n",
    "    if not text:\n",
    "        return []\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"], # Added spaces after punctuation\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# --- Deepseek Processing (Updated max_tokens) ---\n",
    "def gpt_process_text(text_chunk):\n",
    "    # --- Prompt (unchanged) ---\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant responsible for extracting sentences relevant to Ultra-High Performance Concrete (UHPC) from research papers. Your task is to identify, clean, and extract relevant sentences while preserving their original structure.\n",
    "\n",
    "    **Sentence Selection and Cleaning Criteria:**\n",
    "    1️⃣ Extract sentences that explicitly mention 'UHPC' or 'Ultra-High Performance Concrete' (case-insensitive).\n",
    "    2️⃣ Extract sentences that discuss UHPC-related topics even if they do not contain 'UHPC'.\n",
    "    3️⃣ Sentences should be relevant, not generic. Ignore vague statements like “Concrete has high strength.”\n",
    "    4️⃣ Sentences should be at least 8 words long to ensure meaningful content.\n",
    "    5️⃣ Exclude references, captions, formulas, and table data to prevent noisy or incomplete extractions.\n",
    "    6️⃣ Do not modify, rewrite, or summarize extracted sentences. Preserve their original wording, structure, and punctuation.\n",
    "    7️⃣ If a sentence contains references like [1] or (2023), remove only the reference markers, keeping the rest of the sentence intact.\n",
    "    Return the cleaned and validated sentences as a JSON list. Ensure the JSON format is strictly adhered to.\n",
    "\n",
    "    **✅ Output Format (Strict JSON Only):**\n",
    "    ```json\n",
    "    {{\n",
    "        \"valid_sentences\": [\"sentence1\", \"sentence2\", \"sentence3\"]\n",
    "    }}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": text_chunk}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=7800, # Updated max_tokens\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        gpt_output = response.choices[0].message.content.strip()\n",
    "\n",
    "        # --- JSON Parsing Logic (improved robustness slightly) ---\n",
    "        def parse_gpt_response(response_text):\n",
    "            try:\n",
    "                # Attempt direct JSON parsing first, as response_format should ensure it\n",
    "                return json.loads(response_text)\n",
    "            except json.JSONDecodeError as e:\n",
    "                 print(f\"⚠️ 初步 JSON 解析错误: {e}\")\n",
    "                 print(f\"   尝试清理和解析...\")\n",
    "                 try:\n",
    "                    # Fallback: Clean potential markdown and leading/trailing garbage\n",
    "                    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        cleaned_text = json_match.group(0)\n",
    "                        result = json.loads(cleaned_text)\n",
    "                        print(\"   通过正则清理后解析成功。\")\n",
    "                        return result\n",
    "                    else:\n",
    "                        print(f\"   无法在响应中找到有效的 JSON 结构: {response_text[:500]}...\")\n",
    "                        return None\n",
    "                 except json.JSONDecodeError as e2:\n",
    "                    print(f\"   清理后 JSON 解析仍然失败: {e2}\")\n",
    "                    print(f\"   收到内容 (前500字符): {response_text[:500]}...\")\n",
    "                    return None # Give up if parsing fails\n",
    "\n",
    "        result = parse_gpt_response(gpt_output)\n",
    "        return result.get(\"valid_sentences\", []) if result else []\n",
    "\n",
    "    # --- Error Handling (unchanged, but RateLimitError sleep is important) ---\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"❌ Deepseek API 连接失败: {e}\")\n",
    "        return []\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"⏳ Deepseek API 速率限制，等待 15 秒后重试...\") # Increased wait time slightly\n",
    "        time.sleep(15)\n",
    "        return [] # Simplistic handling: skip chunk on rate limit for now\n",
    "    except openai.APIStatusError as e:\n",
    "        # Check for specific error codes, like context length exceeded (though unlikely with current chunk size)\n",
    "        # or output token limit exceeded\n",
    "        print(f\"❌ Deepseek API 状态错误: {e.status_code}\")\n",
    "        print(f\"   响应: {e.response}\")\n",
    "        # Potentially log e.response.text to see the error message from Deepseek\n",
    "        if \"maximum context length\" in str(e.response).lower():\n",
    "             print(\"   错误提示：可能超过了最大上下文长度！\")\n",
    "        if \"maximum number of tokens\" in str(e.response).lower():\n",
    "             print(\"   错误提示：可能超过了最大输出 token 数！输出可能被截断。\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Deepseek 处理失败: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Saving to Doc (unchanged) ---\n",
    "def save_sentences_to_doc(sentences, doc_path):\n",
    "    try:\n",
    "        doc = Document()\n",
    "        doc.add_heading(\"Step1_Text Generation (DeepSeek - Optimized)\", level=1)\n",
    "        count = 0\n",
    "        if isinstance(sentences, list): # Ensure it's a list\n",
    "            for i, sentence in enumerate(sentences, 1):\n",
    "                if isinstance(sentence, str):\n",
    "                    doc.add_paragraph(f\"{i}. {sentence}\")\n",
    "                    count += 1\n",
    "                else:\n",
    "                    print(f\"⚠️ 跳过非字符串项目索引 {i}: {sentence}\")\n",
    "            doc.save(doc_path)\n",
    "            print(f\"✅ 已成功保存 {count} 个句子至 {doc_path}\")\n",
    "        else:\n",
    "             print(f\"❌ 预期句子列表，但收到: {type(sentences)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存 Word 文档失败: {e}\")\n",
    "\n",
    "# --- Main Program Logic (unchanged structure, uses updated defaults) ---\n",
    "def main(pdf_paths, output_doc_path):\n",
    "    all_relevant_sentences = []\n",
    "    total_files = len(pdf_paths)\n",
    "\n",
    "    for idx, pdf_path in enumerate(pdf_paths):\n",
    "        print(f\"\\n🚀 处理文件 {idx+1}/{total_files}: {os.path.basename(pdf_path)}\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"⚠️ 文件未找到，跳过: {pdf_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text = extract_pdf_text(pdf_path)\n",
    "            if not text:\n",
    "                print(f\"⚠️ 未能从 {os.path.basename(pdf_path)} 提取文本或文件为空，跳过。\")\n",
    "                continue\n",
    "\n",
    "            print(\"📖 正在拆分文本 (chunk_size=25000, overlap=500)...\")\n",
    "            # Uses the new defaults from split_text definition\n",
    "            text_chunks = split_text(text)\n",
    "            print(f\"🔹 文本分为 {len(text_chunks)} 个部分。\")\n",
    "\n",
    "            if not text_chunks:\n",
    "                 print(f\"⚠️ 文本拆分后为空，跳过文件 {os.path.basename(pdf_path)}。\")\n",
    "                 continue\n",
    "\n",
    "            chunk_sentences = []\n",
    "            for i, chunk in enumerate(text_chunks):\n",
    "                # Add check for chunk size just in case splitter produces large chunks\n",
    "                if len(chunk) > 30000: # Arbitrary safety check slightly above target\n",
    "                     print(f\"  ⚠️ 块 {i+1} 字符数异常 ({len(chunk)})，可能导致问题。跳过此块。\")\n",
    "                     continue\n",
    "\n",
    "                print(f\"🤖 使用 Deepseek 处理第 {i+1}/{len(text_chunks)} 段文本 (max_tokens=7800)...\")\n",
    "                sentences = gpt_process_text(chunk)\n",
    "                if sentences:\n",
    "                     chunk_sentences.extend(sentences)\n",
    "                time.sleep(1.5) # Slightly increase sleep time due to larger requests\n",
    "\n",
    "            print(f\"📄 文件 {os.path.basename(pdf_path)} 处理完毕，找到 {len(chunk_sentences)} 个相关句子。\")\n",
    "            all_relevant_sentences.extend(chunk_sentences)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 处理文件 {os.path.basename(pdf_path)} 时发生意外错误: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n✅ 所有文件处理完毕，总筛选后的句子数量: {len(all_relevant_sentences)}\")\n",
    "\n",
    "    if all_relevant_sentences:\n",
    "        print(\"📑 保存结果到文档...\")\n",
    "        output_dir = os.path.dirname(output_doc_path)\n",
    "        try:\n",
    "            if output_dir and not os.path.exists(output_dir):\n",
    "                 os.makedirs(output_dir)\n",
    "                 print(f\"   创建输出目录: {output_dir}\")\n",
    "            save_sentences_to_doc(all_relevant_sentences, output_doc_path)\n",
    "        except Exception as e:\n",
    "             print(f\"❌ 创建输出目录或保存文件时出错: {e}\")\n",
    "    else:\n",
    "        print(\"⏹️ 未提取到相关句子，不生成输出文件。\")\n",
    "\n",
    "\n",
    "# --- Execution Block (Path generation unchanged) ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_base_dir = r\"D:\\SIT\\knowledge\\data\"\n",
    "    num_files = 50\n",
    "    if not os.path.isdir(pdf_base_dir):\n",
    "        print(f\"错误：指定的 PDF 基础目录不存在: {pdf_base_dir}\")\n",
    "        exit()\n",
    "    pdf_files = [os.path.join(pdf_base_dir, f\"{i}.pdf\") for i in range(1, num_files + 1)]\n",
    "\n",
    "    output_doc = r\"D:\\SIT\\knowledge\\Result\\Step1_UHPC_Sentences_Extracted_DeepSeek_Optimized.docx\" # Updated output name\n",
    "\n",
    "    print(f\"将处理 {len(pdf_files)} 个 PDF 文件从目录: {pdf_base_dir}\")\n",
    "    print(f\"使用设置: chunk_size=25000, overlap=500, max_tokens=7800\")\n",
    "    print(f\"结果将保存至: {output_doc}\")\n",
    "\n",
    "    main(pdf_files, output_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
