{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f5240-943b-42e8-a5ee-f14971060967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import docx\n",
    "from docx import Document\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 1Ô∏è‚É£ Change 1: Use Deepseek API Key\n",
    "if \"DEEPSEEK_API_KEY\" not in os.environ:\n",
    "    os.environ[\"DEEPSEEK_API_KEY\"] = input(\"Enter your Deepseek API Key: \").strip()\n",
    "\n",
    "# 2Ô∏è‚É£ Read text generated in Step 2 (unchanged)\n",
    "def load_sentences_from_docx(doc_path):\n",
    "    doc = docx.Document(doc_path)\n",
    "    sentences = []\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if text:\n",
    "            text = \" \".join(text.split(\". \")[1:]) if text[0].isdigit() else text\n",
    "            sentences.append(text)\n",
    "    return sentences\n",
    "\n",
    "# 3Ô∏è‚É£ Deepseek generates Cypher code and entities - modified batch logic\n",
    "def generate_cypher_and_entities(sentences, batch_size=10):\n",
    "    results = []  # Store result objects containing cypher and entities\n",
    "    total_batches = (len(sentences) + batch_size - 1) // batch_size  # Round up\n",
    "    \n",
    "    # Create Deepseek client\n",
    "    client = openai.OpenAI(\n",
    "        api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "        base_url=\"https://api.deepseek.com/v1\"\n",
    "    )\n",
    "\n",
    "    # Keep the original detailed prompt template\n",
    "    prompt_template = \"\"\"\n",
    "    You are an expert in knowledge graph construction.\n",
    "    Your task is twofold for the given natural language statement:\n",
    "    1.  Convert the statement into Neo4j Cypher code, primarily using **MERGE** statements to represent the nodes and relationships. Focus on accurately capturing the information within this single sentence.\n",
    "    2.  Extract and list all unique **entity names** that you used as node identifiers (e.g., the value in `{name: 'EntityName'}`) within the generated Cypher code.\n",
    "\n",
    "    Follow these instructions strictly:\n",
    "\n",
    "    ## Identify Entities (Nodes)\n",
    "    - Extract key entities from the input sentence.\n",
    "    - Each entity should have an appropriate label:\n",
    "      - **Material** ‚Üí (`Material`) e.g., `UHPC`, `Silica Fume`\n",
    "      - **Product** ‚Üí (`Product`) e.g., `UHPC-Based Bridge Deck`\n",
    "      - **Process** ‚Üí (`Process`) e.g., `Curing`, `Casting`\n",
    "      - **Property** ‚Üí (`Property`) e.g., `Compressive Strength`, `Durability`\n",
    "      - **Effect/Benefit** ‚Üí (`Benefit`) e.g., `Cost-Effective`, `Reduced Carbon Footprint`\n",
    "      - Use entity names that closely reflect the terms found in the sentence for now. Precise global normalization and deduplication across sentences will be handled in a later step. Base the `name` property primarily on the text.\n",
    "\n",
    "    ## Define Relationships\n",
    "    - Use clear, meaningful relationships between entities.\n",
    "    - Relationship examples:\n",
    "     - **Material Usage** ‚Üí `:USED_IN`\n",
    "     - **Replacement** ‚Üí `:REPLACES`\n",
    "     - **Effects** ‚Üí `:IMPROVES`, `:DECREASES`, `:CONTRIBUTES_TO`, `:EXHIBITS_PROPERTY`\n",
    "     - **Processing Methods** ‚Üí `:HAS_PROCESSING_METHOD`, `:REQUIRES_PROCESS`\n",
    "     - **Testing & Evaluation** ‚Üí `:EVALUATED_BY`\n",
    "     - **Process Execution** ‚Üí `:UNDERGOES`, `:OCCURS_AT`\n",
    "     - (Keep other relevant relationship types you had)\n",
    "\n",
    "    ## Output Format (Strict JSON)\n",
    "    - You **MUST** return the output ONLY as a single, valid JSON object.\n",
    "    - Do NOT include any text, explanations, or markdown formatting before or after the JSON object.\n",
    "    - The JSON object must contain exactly two keys: `\"cypher\"` and `\"entities\"`.\n",
    "    - The value for the `\"cypher\"` key must be a single string containing all the generated Neo4j Cypher code for the input sentence. Use `\\\\n` for newlines within the Cypher string. Use `MERGE` statements.\n",
    "    - The value for the `\"entities\"` key must be a JSON array of strings. This array should list all unique entity names used as the `name` property value within the generated Cypher code (e.g., if you have `{name: \"UHPC\"}` and `{name: \"Silica Fume\"}`, the list should include `\"UHPC\"` and `\"Silica Fume\"`).\n",
    "\n",
    "    ---\n",
    "    ## 4Ô∏è‚É£ Avoid These Common Mistakes\n",
    "    ### 1Ô∏è‚É£ Ambiguity in Relationships\n",
    "    - Relationships must be clearly defined.\n",
    "    - ‚úÖ Example:  \n",
    "      MERGE (glass:Material {name: \"Glass\"})-[:REDUCES]->(internalFriction:Property {name: \"Internal Friction\"})\n",
    "    - ‚ùå Avoid unclear chains like:  \n",
    "      MERGE (glass:Material {name: \"Glass\"})-[:REDUCES]->(internalFriction:Property {name: \"Internal Friction\"})-[:AFFECTS]->(flow:Property {name: \"Flow\"})\n",
    "    - Specify whether the effect is increasing or decreasing.\n",
    "\n",
    "    ---\n",
    "    ### 2Ô∏è‚É£ Errors\n",
    "    - Do not assign incorrect properties to entities.\n",
    "    - ‚úÖ Example:  \n",
    "      MERGE (uhpc:Product {name: \"UHPC\"})-[:HAS_PROCESSING_METHOD]->(hotCuring:Process {name: \"Hot Curing\"})\n",
    "      MERGE (hotCuring)-[:HAS]->(curingTemperature:Property {name: \"Curing Temperature\"})\n",
    "    - ‚ùå Incorrect:  \n",
    "      MERGE (uhpc:Product {name: \"UHPC\"})-[:HAS_PROPERTY]->(curingTemperature:Property {name: \"Curing Temperature\"})\n",
    "\n",
    "    ---\n",
    "    ### 3Ô∏è‚É£ Redundancy\n",
    "    - Avoid multiple nodes for the same concept (e.g., `\"Waste Glass\"`, `\"Glass Particles\"`, `\"Fine Glass\"` ‚Üí should all be `\"Glass\"`).\n",
    "    - Use a **consistent naming convention** for entities.\n",
    "\n",
    "    ---\n",
    "    ### 4Ô∏è‚É£ Inconsistency\n",
    "    - Use **consistent labels** and **relationship types** for the same entity across different sentences.\n",
    "    - ‚úÖ **Correct & Consistent**  \n",
    "      \"Glass sand can be efficiently used to produce UHPC.\"  \n",
    "      MERGE (glassSand:Material {name: \"Glass Sand\"})-[:USED_IN]->(uhpc:Product {name: \"UHPC\"})\n",
    "      \"The glass sand can increase the workability of UHPC.\"  \n",
    "      MERGE (glassSand:Material {name: \"Glass Sand\"})-[:IMPROVES]->(workability:Property {name: \"Workability\"})\n",
    "    - ‚ùå **Incorrect & Inconsistent**  \n",
    "      \"Glass sand can be efficiently used to produce UHPC.\"  \n",
    "      MERGE (uhpc:Product {name: \"UHPC\"})-[:USES]->(glassSand:Material {name: \"Glass Sand\"})\n",
    "      \"The glass sand can increase the workability of UHPC.\"  \n",
    "      MERGE (glassSand:Material {name: \"Glass Sand\"})-[:CONTRIBUTES_TO]->(workability:Property {name: \"Workability\"})\n",
    "\n",
    "    ---\n",
    "    ## 5Ô∏è‚É£ Cypher Code Generation Example\n",
    "    ### Example: Glass Sand in UHPC\n",
    "    **Input:**  \n",
    "    \"Glass sand can be efficiently used to produce UHPC and eliminate the need for quartz sand, yielding a cost-effective and environmentally friendly solution.\"\n",
    "\n",
    "    **Expected Output:**\n",
    "    ```json\n",
    "    {\n",
    "      \"cypher\": \"MERGE (glassSand:Material {name: \\\\\"Glass Sand\\\\\"})\\nMERGE (quartzSand:Material {name: \\\\\"Quartz Sand\\\\\"})\\nMERGE (uhpc:Product {name: \\\\\"UHPC\\\\\"})\\nMERGE (costEffectiveness:Benefit {name: \\\\\"Cost-Effective\\\\\"})\\nMERGE (environmentalBenefit:Benefit {name: \\\\\"Environmentally Friendly\\\\\"})\\n\\nMERGE (glassSand)-[:USED_IN]->(uhpc)\\nMERGE (glassSand)-[:REPLACES]->(quartzSand)\\nMERGE (glassSand)-[:YIELDS]->(costEffectiveness)\\nMERGE (glassSand)-[:YIELDS]->(environmentalBenefit)\",\n",
    "      \"entities\": [\"Glass Sand\", \"Quartz Sand\", \"UHPC\", \"Cost-Effective\", \"Environmentally Friendly\"]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ---\n",
    "    Now, please process the following sentence and provide ONLY a valid JSON object with cypher code and entities list:\n",
    "\n",
    "    \"{sentence}\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Change: Core fix - process each sentence instead of batch processing\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        batch_index = i // batch_size + 1\n",
    "        sentence_in_batch = i % batch_size + 1\n",
    "        print(f\"üîÑ Processing batch {batch_index}/{total_batches} (sentence {i+1}/{len(sentences)})...\")\n",
    "        \n",
    "        # Create prompt for single sentence\n",
    "        single_prompt = prompt_template.replace(\"{sentence}\", sentence)\n",
    "        \n",
    "        try:\n",
    "            # Add retry logic\n",
    "            max_retries = 3\n",
    "            response = None\n",
    "            \n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    # Add timeout setting\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"deepseek-chat\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": single_prompt}],\n",
    "                        temperature=0.2,  # Keep original temperature\n",
    "                        max_tokens=8000,  # Keep large token limit\n",
    "                        top_p=0.95,\n",
    "                        timeout=60  # Add timeout\n",
    "                    )\n",
    "                    break  # Successfully got response, break retry loop\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        print(f\"‚ö†Ô∏è Attempt {attempt+1} failed: {str(e)}, retrying in 5 seconds...\")\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        raise e  # Reached max retries, raise exception\n",
    "            \n",
    "            if response:\n",
    "                response_content = response.choices[0].message.content.strip()\n",
    "                \n",
    "                # Fix: more robust JSON parsing\n",
    "                result = parse_single_response(response_content, sentence)\n",
    "                if result:\n",
    "                    print(f\"‚úÖ Sentence {i+1} generated successfully!\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Sentence {i+1} parsing failed, using empty result\")\n",
    "                    result = {\"sentence\": sentence, \"cypher\": \"\", \"entities\": []}\n",
    "                \n",
    "                results.append(result)\n",
    "            \n",
    "            # Batch success stats\n",
    "            if (i + 1) % batch_size == 0 or i == len(sentences) - 1:\n",
    "                start_idx = (batch_index - 1) * batch_size\n",
    "                end_idx = min(start_idx + batch_size, len(sentences))\n",
    "                curr_batch = results[start_idx:end_idx]\n",
    "                valid_count = sum(1 for r in curr_batch if r.get('cypher') and r.get('entities'))\n",
    "                print(f\"‚úÖ Batch {batch_index} results: {valid_count}/{len(curr_batch)} sentences valid\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Sentence {i+1} processing failed: {str(e)}\")\n",
    "            # Add empty result on failure to keep index aligned\n",
    "            results.append({\"sentence\": sentence, \"cypher\": \"\", \"entities\": []})\n",
    "        \n",
    "        # Pause after each sentence to avoid API throttling\n",
    "        if i < len(sentences) - 1:\n",
    "            print(\"‚è±Ô∏è Pausing 2 seconds to avoid API throttling...\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        # Longer pause after each completed batch\n",
    "        if (i + 1) % batch_size == 0 and i < len(sentences) - 1:\n",
    "            print(f\"‚è±Ô∏è Finished batch {batch_index}/{total_batches}, pausing 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    return results\n",
    "\n",
    "# New function: parse single sentence response\n",
    "def parse_single_response(response_text, original_sentence):\n",
    "    # Remove markdown code block markers\n",
    "    cleaned_text = response_text\n",
    "    if \"```json\" in cleaned_text:\n",
    "        cleaned_text = cleaned_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # Try direct JSON parse\n",
    "    try:\n",
    "        data = json.loads(cleaned_text)\n",
    "        if isinstance(data, dict) and \"cypher\" in data and \"entities\" in data:\n",
    "            # Add original sentence for tracking\n",
    "            data[\"sentence\"] = original_sentence\n",
    "            return data\n",
    "    except json.JSONDecodeError:\n",
    "        # If direct parse fails, try regex extraction\n",
    "        json_pattern = r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}'\n",
    "        matches = re.finditer(json_pattern, cleaned_text)\n",
    "        \n",
    "        for match in matches:\n",
    "            potential_json = match.group(0)\n",
    "            try:\n",
    "                data = json.loads(potential_json)\n",
    "                if \"cypher\" in data and \"entities\" in data:\n",
    "                    data[\"sentence\"] = original_sentence\n",
    "                    return data\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # If not found, try to repair common issues\n",
    "    try:\n",
    "        fixed_text = re.sub(r'[\\n\\r\\t]', ' ', cleaned_text)\n",
    "        fixed_text = re.sub(r'\\\\(?![\"\\\\/bfnrt])', r'\\\\\\\\', fixed_text)  # Fix bad escape chars\n",
    "        \n",
    "        start_idx = fixed_text.find('{')\n",
    "        if start_idx != -1:\n",
    "            stack = []\n",
    "            for i, char in enumerate(fixed_text[start_idx:], start_idx):\n",
    "                if char == '{':\n",
    "                    stack.append('{')\n",
    "                elif char == '}':\n",
    "                    stack.pop()\n",
    "                    if not stack:\n",
    "                        json_str = fixed_text[start_idx:i+1]\n",
    "                        try:\n",
    "                            data = json.loads(json_str)\n",
    "                            if \"cypher\" in data and \"entities\" in data:\n",
    "                                data[\"sentence\"] = original_sentence\n",
    "                                return data\n",
    "                        except:\n",
    "                            pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Final attempt: manually construct basic JSON\n",
    "    try:\n",
    "        cypher_pattern = r'\"cypher\"\\s*:\\s*\"([^\"]*(?:\\\\.[^\"]*)*)\"'\n",
    "        entities_pattern = r'\"entities\"\\s*:\\s*\\[(.*?)\\]'\n",
    "        \n",
    "        cypher_match = re.search(cypher_pattern, cleaned_text)\n",
    "        entities_match = re.search(entities_pattern, cleaned_text)\n",
    "        \n",
    "        if cypher_match:\n",
    "            cypher_code = cypher_match.group(1)\n",
    "            entities = []\n",
    "            \n",
    "            if entities_match:\n",
    "                entities_text = entities_match.group(1)\n",
    "                entity_pattern = r'\"([^\"]*(?:\\\\.[^\"]*)*)\"'\n",
    "                entities = re.findall(entity_pattern, entities_text)\n",
    "            \n",
    "            return {\n",
    "                \"sentence\": original_sentence,\n",
    "                \"cypher\": cypher_code,\n",
    "                \"entities\": entities\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 4Ô∏è‚É£ Save Cypher code and entities to different files\n",
    "def save_results_to_files(results, cypher_doc_path, entities_doc_path, total_sentences):\n",
    "    # Save Cypher code\n",
    "    cypher_doc = Document()\n",
    "    cypher_doc.add_heading(\"Step3_Cypher_Code_Generation\", level=1)\n",
    "    \n",
    "    # Add summary info\n",
    "    valid_results = [r for r in results if r.get('cypher')]\n",
    "    cypher_doc.add_paragraph(f\"Total sentences: {total_sentences}\")\n",
    "    cypher_doc.add_paragraph(f\"Number of generated Cypher codes: {len(valid_results)}\")\n",
    "    cypher_doc.add_paragraph(f\"Generation rate: {(len(valid_results)/total_sentences*100):.2f}%\")\n",
    "    cypher_doc.add_paragraph(\"---\")\n",
    "    \n",
    "    # Add each Cypher code\n",
    "    for i, result in enumerate(results, 1):\n",
    "        if 'sentence' in result:\n",
    "            cypher_doc.add_paragraph(f\"Sentence {i}: {result['sentence']}\")\n",
    "        \n",
    "        if result.get('cypher'):\n",
    "            cypher_code = result['cypher'].replace('\\\\n', '\\n')\n",
    "            cypher_doc.add_paragraph(f\"Cypher {i}: {cypher_code}\")\n",
    "        else:\n",
    "            cypher_doc.add_paragraph(f\"Cypher {i}: [No valid code generated]\")\n",
    "        \n",
    "        cypher_doc.add_paragraph(\"---\")\n",
    "    \n",
    "    cypher_doc.save(cypher_doc_path)\n",
    "    print(f\"‚úÖ Cypher code successfully saved to {cypher_doc_path}\")\n",
    "    \n",
    "    # Save entities\n",
    "    entities_doc = Document()\n",
    "    entities_doc.add_heading(\"Step3_Extracted_Entities\", level=1)\n",
    "    \n",
    "    all_entities = []\n",
    "    for result in results:\n",
    "        if result.get('entities'):\n",
    "            all_entities.extend(result['entities'])\n",
    "    \n",
    "    unique_entities = sorted(set(all_entities))\n",
    "    \n",
    "    # Add summary info\n",
    "    entities_doc.add_paragraph(f\"Total sentences: {total_sentences}\")\n",
    "    entities_doc.add_paragraph(f\"Extracted entities: {len(all_entities)}\")\n",
    "    entities_doc.add_paragraph(f\"Unique entities: {len(unique_entities)}\")\n",
    "    entities_doc.add_paragraph(\"---\")\n",
    "    \n",
    "    # Add entities by sentence\n",
    "    entities_doc.add_heading(\"Entities by Sentence:\", level=2)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        if 'sentence' in result:\n",
    "            entities_doc.add_paragraph(f\"Sentence {i}: {result['sentence']}\")\n",
    "        \n",
    "        if result.get('entities'):\n",
    "            entity_list = \", \".join(result['entities'])\n",
    "            entities_doc.add_paragraph(f\"Entities {i}: {entity_list}\")\n",
    "        else:\n",
    "            entities_doc.add_paragraph(f\"Entities {i}: [No entities extracted]\")\n",
    "        \n",
    "        entities_doc.add_paragraph(\"---\")\n",
    "    \n",
    "    # Add unique entity list\n",
    "    entities_doc.add_heading(\"Unique Entities:\", level=2)\n",
    "    for i, entity in enumerate(unique_entities, 1):\n",
    "        entities_doc.add_paragraph(f\"{i}. {entity}\")\n",
    "    \n",
    "    entities_doc.save(entities_doc_path)\n",
    "    print(f\"‚úÖ Entities successfully saved to {entities_doc_path}\")\n",
    "    \n",
    "    # Save complete results as JSON\n",
    "    json_path = os.path.splitext(cypher_doc_path)[0] + \"_complete.json\"\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Complete results saved as JSON: {json_path}\")\n",
    "\n",
    "# 5Ô∏è‚É£ Main program (restructured to process sentences one by one)\n",
    "def main(input_doc_path, cypher_output_path, entities_output_path, batch_size=10):\n",
    "    print(\"üìÇ Reading input file...\")\n",
    "    sentences = load_sentences_from_docx(input_doc_path)\n",
    "    print(f\"üìú Number of original sentences: {len(sentences)}\")\n",
    "\n",
    "    print(f\"ü§ñ Starting Cypher code generation and entity extraction...\")\n",
    "    results = generate_cypher_and_entities(sentences, batch_size)\n",
    "    \n",
    "    valid_cypher_count = sum(1 for r in results if r.get('cypher'))\n",
    "    valid_entities_count = sum(1 for r in results if r.get('entities') and len(r['entities']) > 0)\n",
    "    \n",
    "    print(f\"üîç Valid Cypher code generation rate: {valid_cypher_count}/{len(sentences)} ({(valid_cypher_count/len(sentences)*100):.2f}%)\")\n",
    "    print(f\"üìä Valid entity extraction rate: {valid\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
