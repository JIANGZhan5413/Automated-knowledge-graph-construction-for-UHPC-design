{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821b42b-0d08-460a-b23d-ffadf53a3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import docx\n",
    "from docx import Document\n",
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class KnowledgeGraphGenerator:\n",
    "    \"\"\"Knowledge Graph Generator - Converts natural language to Neo4j Cypher code and extracts entities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize generator and setup API key\"\"\"\n",
    "        self.setup_api_key()\n",
    "        self.client = openai.OpenAI(\n",
    "            api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "        self.canonical_mapping = {}\n",
    "        \n",
    "    def setup_api_key(self):\n",
    "        \"\"\"Setup Deepseek API key\"\"\"\n",
    "        if \"DEEPSEEK_API_KEY\" not in os.environ:\n",
    "            api_key = input(\"Enter your Deepseek API Key: \").strip()\n",
    "            os.environ[\"DEEPSEEK_API_KEY\"] = api_key\n",
    "    \n",
    "    def load_canonical_mapping(self, mapping_file_path: str) -> Dict:\n",
    "        \"\"\"Load canonical mapping file for entity standardization\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(mapping_file_path):\n",
    "                with open(mapping_file_path, 'r', encoding='utf-8') as f:\n",
    "                    self.canonical_mapping = json.load(f)\n",
    "                print(f\"‚úÖ Canonical mapping loaded: {len(self.canonical_mapping)} entries\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Canonical mapping file not found: {mapping_file_path}\")\n",
    "                print(\"üìù Creating empty mapping file...\")\n",
    "                self.canonical_mapping = {}\n",
    "                with open(mapping_file_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump({}, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading canonical mapping: {e}\")\n",
    "            self.canonical_mapping = {}\n",
    "        \n",
    "        return self.canonical_mapping\n",
    "    \n",
    "    def load_sentences_from_docx(self, doc_path: str) -> List[str]:\n",
    "        \"\"\"Load sentences from Word document\"\"\"\n",
    "        try:\n",
    "            doc = docx.Document(doc_path)\n",
    "            sentences = []\n",
    "            \n",
    "            for para in doc.paragraphs:\n",
    "                text = para.text.strip()\n",
    "                if text:\n",
    "                    # Remove numbered prefix if exists\n",
    "                    if text and text[0].isdigit() and '. ' in text:\n",
    "                        text = \". \".join(text.split(\". \")[1:])\n",
    "                    sentences.append(text)\n",
    "            \n",
    "            print(f\"‚úÖ Successfully loaded {len(sentences)} sentences\")\n",
    "            return sentences\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to read document: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_prompt_template(self) -> str:\n",
    "        \"\"\"Get prompt template with canonical mapping instructions\"\"\"\n",
    "        mapping_str = json.dumps(self.canonical_mapping, indent=2) if self.canonical_mapping else \"{}\"\n",
    "        \n",
    "        return \"\"\"You are an expert in knowledge graph construction.\n",
    "Your task is twofold for the given natural language statement:\n",
    "1. Convert the statement into Neo4j Cypher code, primarily using **MERGE** statements to represent the nodes and relationships. Focus on accurately capturing the information within this single sentence.\n",
    "2. Extract and list all unique **entity names** that you used as node identifiers (e.g., the value in `{name: 'EntityName'}`) within the generated Cypher code.\n",
    "\n",
    "Follow these instructions strictly:\n",
    "\n",
    "## Canonical Entity Standardization\n",
    "This procedure standardizes node names in Cypher code so that the graph stores a single, canonical representation of every concept.\n",
    "\n",
    "**Input:**\n",
    "1. A canonical-mapping file (JSON format below)\n",
    "2. The initial Cypher script with MERGE statements\n",
    "\n",
    "**Canonical Mapping:**\n",
    "\"\"\" + mapping_str + \"\"\"\n",
    "\n",
    "**Method:**\n",
    "1. Sequentially inspect every MERGE statement in the script and record the string assigned to each name property.\n",
    "2. For every recorded name, consult the canonical-mapping file:\n",
    "   a) If the name appears in a variant list, substitute the corresponding canonical term in the Cypher statement.\n",
    "   b) If no mapping is found, preserve the original wording on the assumption that it is already canonical.\n",
    "3. Retain without alteration the variable identifiers, node labels, relationship types, directions and the overall ordering of statements; only the literal contents of the name properties are subject to change.\n",
    "4. Assemble the modified statements into a single, syntactically valid Cypher block.\n",
    "\n",
    "## Output Format (Strict JSON)\n",
    "- You **MUST** return the output ONLY as a single, valid JSON object.\n",
    "- Do NOT include any text, explanations, or markdown formatting before or after the JSON object.\n",
    "- The JSON object must contain exactly two keys: `\"cypher\"` and `\"entities\"`.\n",
    "- The value for the `\"cypher\"` key must be a single string containing all the generated Neo4j Cypher code for the input sentence. Use `\\\\n` for newlines within the Cypher string. Use `MERGE` statements.\n",
    "- The value for the `\"entities\"` key must be a JSON array of strings. This array should list all unique entity names used as the `name` property value within the generated Cypher code.\n",
    "\n",
    "## Avoid These Common Mistakes\n",
    "\n",
    "### 1. Ambiguity in Relationships\n",
    "- Relationships must be clearly defined.\n",
    "- ‚úÖ Example:  \n",
    "  MERGE (glass:Material {{name: \"Glass\"}})-[:REDUCES]->(internalFriction:Property {{name: \"Internal Friction\"}})\n",
    "- ‚ùå Avoid unclear chains like:  \n",
    "  MERGE (glass:Material {{name: \"Glass\"}})-[:REDUCES]->(internalFriction:Property {{name: \"Internal Friction\"}})-[:AFFECTS]->(flow:Property {{name: \"Flow\"}})\n",
    "- Specify whether the effect is increasing or decreasing.\n",
    "\n",
    "### 2. Property Assignment Errors\n",
    "- Do not assign incorrect properties to entities.\n",
    "- ‚úÖ Example:  \n",
    "  MERGE (uhpc:Product {{name: \"UHPC\"}})-[:HAS_PROCESSING_METHOD]->(hotCuring:Process {{name: \"Hot Curing\"}})\n",
    "  MERGE (hotCuring)-[:HAS]->(curingTemperature:Property {{name: \"Curing Temperature\"}})\n",
    "- ‚ùå Incorrect:  \n",
    "  MERGE (uhpc:Product {{name: \"UHPC\"}})-[:HAS_PROPERTY]->(curingTemperature:Property {{name: \"Curing Temperature\"}})\n",
    "\n",
    "### 3. Redundancy\n",
    "- Avoid multiple nodes for the same concept (e.g., `\"Waste Glass\"`, `\"Glass Particles\"`, `\"Fine Glass\"` ‚Üí should all be `\"Glass\"`).\n",
    "- Use a **consistent naming convention** for entities.\n",
    "\n",
    "### 4. Inconsistency\n",
    "- Use **consistent labels** and **relationship types** for the same entity across different sentences.\n",
    "\n",
    "## Example\n",
    "**Input:**  \n",
    "\"Glass sand can be efficiently used to produce UHPC and eliminate the need for quartz sand, yielding a cost-effective and environmentally friendly solution.\"\n",
    "\n",
    "**Expected Output:**\n",
    "{{\n",
    "  \"cypher\": \"MERGE (glassSand:Material {{name: \\\\\"Glass Sand\\\\\"}})\\\\nMERGE (quartzSand:Material {{name: \\\\\"Quartz Sand\\\\\"}})\\\\nMERGE (uhpc:Product {{name: \\\\\"UHPC\\\\\"}})\\\\nMERGE (costEffectiveness:Benefit {{name: \\\\\"Cost-Effective\\\\\"}})\\\\nMERGE (environmentalBenefit:Benefit {{name: \\\\\"Environmentally Friendly\\\\\"}})\\\\n\\\\nMERGE (glassSand)-[:USED_IN]->(uhpc)\\\\nMERGE (glassSand)-[:REPLACES]->(quartzSand)\\\\nMERGE (glassSand)-[:YIELDS]->(costEffectiveness)\\\\nMERGE (glassSand)-[:YIELDS]->(environmentalBenefit)\",\n",
    "  \"entities\": [\"Glass Sand\", \"Quartz Sand\", \"UHPC\", \"Cost-Effective\", \"Environmentally Friendly\"]\n",
    "}}\n",
    "\n",
    "Now, please process the following sentence and provide ONLY a valid JSON object with cypher code and entities list:\n",
    "\n",
    "\"{sentence}\"\n",
    "\"\"\"\n",
    "    \n",
    "    def parse_response(self, response_text: str, original_sentence: str) -> Optional[Dict]:\n",
    "        \"\"\"Parse API response to extract Cypher code and entities\"\"\"\n",
    "        # Clean response text\n",
    "        cleaned_text = response_text.strip()\n",
    "        \n",
    "        # Remove code block markers\n",
    "        if \"```json\" in cleaned_text:\n",
    "            cleaned_text = cleaned_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        # Try direct JSON parsing\n",
    "        try:\n",
    "            data = json.loads(cleaned_text)\n",
    "            if self.validate_response(data):\n",
    "                data[\"sentence\"] = original_sentence\n",
    "                return data\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        # If direct parsing fails, try regex extraction\n",
    "        json_match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                data = json.loads(json_match.group())\n",
    "                if self.validate_response(data):\n",
    "                    data[\"sentence\"] = original_sentence\n",
    "                    return data\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        # Try to extract cypher and entities separately\n",
    "        try:\n",
    "            cypher_pattern = r'\"cypher\"\\s*:\\s*\"([^\"]*(?:\\\\.[^\"]*)*)\"'\n",
    "            entities_pattern = r'\"entities\"\\s*:\\s*\\[(.*?)\\]'\n",
    "            \n",
    "            cypher_match = re.search(cypher_pattern, cleaned_text, re.DOTALL)\n",
    "            entities_match = re.search(entities_pattern, cleaned_text)\n",
    "            \n",
    "            if cypher_match:\n",
    "                cypher_code = cypher_match.group(1)\n",
    "                entities = []\n",
    "                \n",
    "                if entities_match:\n",
    "                    entities_text = entities_match.group(1)\n",
    "                    entity_pattern = r'\"([^\"]*(?:\\\\.[^\"]*)*)\"'\n",
    "                    entities = re.findall(entity_pattern, entities_text)\n",
    "                \n",
    "                return {\n",
    "                    \"sentence\": original_sentence,\n",
    "                    \"cypher\": cypher_code,\n",
    "                    \"entities\": entities\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Parsing error: {e}\")\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è Failed to parse response: {response_text[:100]}...\")\n",
    "        return None\n",
    "    \n",
    "    def validate_response(self, data: Dict) -> bool:\n",
    "        \"\"\"Validate response data\"\"\"\n",
    "        return (isinstance(data, dict) and \n",
    "                \"cypher\" in data and \n",
    "                \"entities\" in data and\n",
    "                isinstance(data[\"cypher\"], str) and\n",
    "                isinstance(data[\"entities\"], list))\n",
    "    \n",
    "    def process_sentence(self, sentence: str, retry_count: int = 3) -> Optional[Dict]:\n",
    "        \"\"\"Process single sentence to generate Cypher code and entities\"\"\"\n",
    "        prompt = self.get_prompt_template().format(sentence=sentence)\n",
    "        \n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"deepseek-chat\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=4000,\n",
    "                    timeout=60\n",
    "                )\n",
    "                \n",
    "                response_content = response.choices[0].message.content.strip()\n",
    "                result = self.parse_response(response_content, sentence)\n",
    "                \n",
    "                if result:\n",
    "                    return result\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Attempt {attempt + 1} parsing failed\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Attempt {attempt + 1} API call failed: {e}\")\n",
    "                if attempt < retry_count - 1:\n",
    "                    time.sleep(5)  # Wait before retry\n",
    "        \n",
    "        return {\"sentence\": sentence, \"cypher\": \"\", \"entities\": []}\n",
    "    \n",
    "    def process_all_sentences(self, sentences: List[str]) -> List[Dict]:\n",
    "        \"\"\"Process all sentences\"\"\"\n",
    "        results = []\n",
    "        total = len(sentences)\n",
    "        \n",
    "        print(f\"üöÄ Starting to process {total} sentences...\")\n",
    "        \n",
    "        for i, sentence in enumerate(sentences, 1):\n",
    "            print(f\"üìù Processing sentence {i}/{total}: {sentence[:50]}...\")\n",
    "            \n",
    "            result = self.process_sentence(sentence)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Show processing result\n",
    "            if result and result.get('cypher') and result.get('entities'):\n",
    "                print(f\"‚úÖ Sentence {i} processed successfully\")\n",
    "            else:\n",
    "                print(f\"‚ùå Sentence {i} processing failed\")\n",
    "            \n",
    "            # API rate limiting\n",
    "            if i < total:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Statistics\n",
    "        successful = sum(1 for r in results if r.get('cypher') and r.get('entities'))\n",
    "        success_rate = (successful / total) * 100\n",
    "        print(f\"üìä Processing complete: {successful}/{total} ({success_rate:.1f}%)\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_results(self, results: List[Dict], cypher_path: str, entities_path: str):\n",
    "        \"\"\"Save results to Word documents\"\"\"\n",
    "        self.save_cypher_results(results, cypher_path)\n",
    "        self.save_entities_results(results, entities_path)\n",
    "        self.save_json_results(results, cypher_path.replace('.docx', '.json'))\n",
    "    \n",
    "    def save_cypher_results(self, results: List[Dict], path: str):\n",
    "        \"\"\"Save Cypher code results to Word document\"\"\"\n",
    "        doc = Document()\n",
    "        doc.add_heading(\"Cypher Code Generation Results\", level=1)\n",
    "        \n",
    "        # Add statistics\n",
    "        total = len(results)\n",
    "        successful = sum(1 for r in results if r.get('cypher'))\n",
    "        doc.add_paragraph(f\"Total sentences: {total}\")\n",
    "        doc.add_paragraph(f\"Successfully generated: {successful}\")\n",
    "        doc.add_paragraph(f\"Success rate: {(successful/total*100):.1f}%\")\n",
    "        doc.add_paragraph(\"=\" * 50)\n",
    "        \n",
    "        # Add each result\n",
    "        for i, result in enumerate(results, 1):\n",
    "            doc.add_heading(f\"Sentence {i}\", level=2)\n",
    "            doc.add_paragraph(f\"Original: {result.get('sentence', '')}\")\n",
    "            \n",
    "            cypher = result.get('cypher', '')\n",
    "            if cypher:\n",
    "                # Format Cypher code\n",
    "                formatted_cypher = cypher.replace('\\\\n', '\\n')\n",
    "                doc.add_paragraph(\"Cypher Code:\")\n",
    "                doc.add_paragraph(formatted_cypher, style='Normal')\n",
    "            else:\n",
    "                doc.add_paragraph(\"Cypher Code: [Generation failed]\")\n",
    "            \n",
    "            doc.add_paragraph(\"-\" * 30)\n",
    "        \n",
    "        doc.save(path)\n",
    "        print(f\"‚úÖ Cypher results saved to: {path}\")\n",
    "    \n",
    "    def save_entities_results(self, results: List[Dict], path: str):\n",
    "        \"\"\"Save entities results to Word document\"\"\"\n",
    "        doc = Document()\n",
    "        doc.add_heading(\"Entity Extraction Results\", level=1)\n",
    "        \n",
    "        # Collect all entities\n",
    "        all_entities = []\n",
    "        for result in results:\n",
    "            entities = result.get('entities', [])\n",
    "            all_entities.extend(entities)\n",
    "        \n",
    "        unique_entities = sorted(set(all_entities))\n",
    "        \n",
    "        # Add statistics\n",
    "        doc.add_paragraph(f\"Total sentences: {len(results)}\")\n",
    "        doc.add_paragraph(f\"Total extracted entities: {len(all_entities)}\")\n",
    "        doc.add_paragraph(f\"Unique entities: {len(unique_entities)}\")\n",
    "        doc.add_paragraph(\"=\" * 50)\n",
    "        \n",
    "        # Show entities by sentence\n",
    "        doc.add_heading(\"Entities by Sentence\", level=2)\n",
    "        for i, result in enumerate(results, 1):\n",
    "            doc.add_paragraph(f\"Sentence {i}: {result.get('sentence', '')}\")\n",
    "            entities = result.get('entities', [])\n",
    "            if entities:\n",
    "                doc.add_paragraph(f\"Entities: {', '.join(entities)}\")\n",
    "            else:\n",
    "                doc.add_paragraph(\"Entities: [None]\")\n",
    "            doc.add_paragraph(\"\")\n",
    "        \n",
    "        # Unique entities list\n",
    "        doc.add_heading(\"All Unique Entities\", level=2)\n",
    "        for i, entity in enumerate(unique_entities, 1):\n",
    "            doc.add_paragraph(f\"{i}. {entity}\")\n",
    "        \n",
    "        doc.save(path)\n",
    "        print(f\"‚úÖ Entity results saved to: {path}\")\n",
    "    \n",
    "    def save_json_results(self, results: List[Dict], path: str):\n",
    "        \"\"\"Save complete results as JSON\"\"\"\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Complete results saved to: {path}\")\n",
    "    \n",
    "    def run(self, input_doc_path: str, cypher_output_path: str, entities_output_path: str, \n",
    "            canonical_mapping_path: str = \"canonical_mapping.json\"):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        print(\"üîÑ Starting Knowledge Graph Generation Pipeline...\")\n",
    "        \n",
    "        # 1. Load canonical mapping\n",
    "        self.load_canonical_mapping(canonical_mapping_path)\n",
    "        \n",
    "        # 2. Load sentences\n",
    "        sentences = self.load_sentences_from_docx(input_doc_path)\n",
    "        if not sentences:\n",
    "            print(\"‚ùå No sentences loaded, terminating program\")\n",
    "            return\n",
    "        \n",
    "        # 3. Process all sentences\n",
    "        results = self.process_all_sentences(sentences)\n",
    "        \n",
    "        # 4. Save results\n",
    "        self.save_results(results, cypher_output_path, entities_output_path)\n",
    "        \n",
    "        print(\"üéâ Knowledge Graph Generation Pipeline Complete!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Configure file paths\n",
    "    input_doc =   # Input sentences document\n",
    "    cypher_output =   # Cypher code output\n",
    "    entities_output =   # Entities output\n",
    "    canonical_mapping =   # Canonical mapping file\n",
    "    \n",
    "    # Create generator and run\n",
    "    generator = KnowledgeGraphGenerator()\n",
    "    generator.run(input_doc, cypher_output, entities_output, canonical_mapping)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424ccc1-123b-4241-957f-3501354db6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
